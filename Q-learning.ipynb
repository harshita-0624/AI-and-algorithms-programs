{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNo0Olira2GmJwTjeCtcXtT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"m406SeNpIk7K","executionInfo":{"status":"ok","timestamp":1741704811826,"user_tz":-330,"elapsed":607,"user":{"displayName":"Harshita R","userId":"00470181867635101216"}}},"outputs":[],"source":["import numpy as np\n","import random\n","\n","# Define the gridworld environment\n","class GridWorld:\n","    def __init__(self):\n","        self.grid = np.array([\n","            [0, 0, 0, 1],  # Goal at (0, 3)\n","            [0, -1, 0, 0],  # Wall with reward -1\n","            [0, 0, 0, 0],\n","            [0, 0, 0, 0]  # Start at (3, 0)\n","        ])\n","        self.start_state = (3, 0)\n","        self.state = self.start_state\n","\n","    def reset(self):\n","        self.state = self.start_state\n","        return self.state\n","\n","    def is_terminal(self, state):\n","        return self.grid[state] == 1 or self.grid[state] == -1\n","\n","    def get_next_state(self, state, action):\n","        next_state = list(state)\n","        if action == 0:  # Move up\n","            next_state[0] = max(0, state[0] - 1)\n","        elif action == 1:  # Move right\n","            next_state[1] = min(3, state[1] + 1)\n","        elif action == 2:  # Move down\n","            next_state[0] = min(3, state[0] + 1)\n","        elif action == 3:  # Move left\n","            next_state[1] = max(0, state[1] - 1)\n","        return tuple(next_state)\n","\n","    def step(self, action):\n","        next_state = self.get_next_state(self.state, action)\n","        reward = self.grid[next_state]\n","        self.state = next_state\n","        done = self.is_terminal(next_state)\n","        return next_state, reward, done"]},{"cell_type":"code","source":["class QLearningAgent:\n","    def __init__(self, learning_rate=0.1, discount_factor=0.9, exploration_rate=0.1):\n","        self.q_table = np.zeros((4, 4, 4))  # Q-values for each state-action pair\n","        self.learning_rate = learning_rate\n","        self.discount_factor = discount_factor\n","        self.exploration_rate = exploration_rate\n","\n","    def choose_action(self, state):\n","        if random.uniform(0, 1) < self.exploration_rate:\n","            return random.randint(0, 3)  # Explore\n","        else:\n","            return np.argmax(self.q_table[state])  # Exploit\n","\n","    def update_q_value(self, state, action, reward, next_state):\n","        max_future_q = np.max(self.q_table[next_state])  # Best Q-value for next state\n","        current_q = self.q_table[state][action]\n","        # Q-learning formula\n","        self.q_table[state][action] = current_q + self.learning_rate * (\n","            reward + self.discount_factor * max_future_q - current_q\n","        )"],"metadata":{"id":"lLvbH4JiJaYp","executionInfo":{"status":"ok","timestamp":1741704830404,"user_tz":-330,"elapsed":549,"user":{"displayName":"Harshita R","userId":"00470181867635101216"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["env = GridWorld()\n","agent = QLearningAgent()\n","\n","episodes = 1000  # Number of training episodes\n","\n","for episode in range(episodes):\n","    state = env.reset()  # Reset the environment at the start of each episode\n","    done = False\n","\n","    while not done:\n","        action = agent.choose_action(state)  # Choose an action\n","        next_state, reward, done = env.step(action)  # Take the action and observe next state, reward\n","        agent.update_q_value(state, action, reward, next_state)  # Update Q-values\n","        state = next_state  # Move to the next state"],"metadata":{"id":"eX8hIMTMJfSQ","executionInfo":{"status":"ok","timestamp":1741704834695,"user_tz":-330,"elapsed":6,"user":{"displayName":"Harshita R","userId":"00470181867635101216"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["print(\"\\nFinal Q-Table after training:\")\n","print(agent.q_table)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I3ww7xBFru65","executionInfo":{"status":"ok","timestamp":1741704837656,"user_tz":-330,"elapsed":612,"user":{"displayName":"Harshita R","userId":"00470181867635101216"}},"outputId":"e0108b38-2755-4143-97b0-ddc8bfe2215d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Final Q-Table after training:\n","[[[ 7.10394511e-01  8.10000000e-01  6.17742392e-01  6.70627290e-01]\n","  [ 6.59905364e-01  9.00000000e-01 -8.78423345e-01  6.11958250e-01]\n","  [ 8.01522770e-01  1.00000000e+00  7.27334078e-01  7.36863483e-01]\n","  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n","\n"," [[ 7.29000000e-01 -7.94108868e-01  5.35717755e-01  6.03478213e-01]\n","  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n","  [ 8.99580361e-01  3.09510000e-02  4.98611247e-02 -1.00000000e-01]\n","  [ 4.68559000e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n","\n"," [[ 6.56100000e-01  4.34590991e-01  4.96902229e-01  5.49572012e-01]\n","  [-1.00000000e-01  6.87216977e-01  0.00000000e+00  0.00000000e+00]\n","  [ 7.97462914e-01  6.93360000e-03  0.00000000e+00  1.34702198e-01]\n","  [ 7.92189000e-02  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n","\n"," [[ 5.90490000e-01  2.38702818e-01  5.02647627e-01  4.29490451e-01]\n","  [ 4.67547365e-01  1.66970632e-04  0.00000000e+00  0.00000000e+00]\n","  [ 1.11076323e-02  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n","  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]]\n"]}]},{"cell_type":"code","source":["state = env.reset()\n","done = False\n","print(\"\\nTesting agent after training...\\n\")\n","\n","while not done:\n","    action = np.argmax(agent.q_table[state])  # Choose best action based on learned Q-values\n","    next_state, reward, done = env.step(action)\n","    print(f\"State: {state}, Action: {action}, Reward: {reward}\")\n","    state = next_state\n","\n","print(\"\\nAgent reached terminal state.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B463kiMBs5j8","executionInfo":{"status":"ok","timestamp":1741708463234,"user_tz":-330,"elapsed":19,"user":{"displayName":"Harshita R","userId":"00470181867635101216"}},"outputId":"773ff352-ddbc-4ee5-9b0f-099c94d2e504"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Testing agent after training...\n","\n","State: (3, 0), Action: 0, Reward: 0\n","State: (2, 0), Action: 0, Reward: 0\n","State: (1, 0), Action: 0, Reward: 0\n","State: (0, 0), Action: 1, Reward: 0\n","State: (0, 1), Action: 1, Reward: 0\n","State: (0, 2), Action: 1, Reward: 1\n","\n","Agent reached terminal state.\n"]}]}]}